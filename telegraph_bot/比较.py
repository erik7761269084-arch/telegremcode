import requests
from bs4 import BeautifulSoup
from telegraph import Telegraph
from datetime import datetime
import re
import os
import sys
from telegraph.exceptions import TelegraphException


switch_del_content_words = True   #åˆ é™¤å†…å®¹æ–‡å­—ï¼Œåªä¿ç•™å›¾ç‰‡

# è¦å¤åˆ¶çš„ telegraph é¡µé¢åˆ—è¡¨
SOURCE_URLS = []
TXT_FILE = r"D:\project\pythonLL\telegraph_links_ç¦åˆ©å§¬.txt"
# ä¼˜å…ˆä» txt æ–‡ä»¶è¯»å–
if TXT_FILE and os.path.exists(TXT_FILE):
    with open(TXT_FILE, "r", encoding="utf-8") as f:
        SOURCE_URLS = [line.strip() for line in f if line.strip()]
else:
    # å¦‚æœæ²¡æœ‰ txtï¼Œå°±ç”¨å†…ç½®çš„åˆ—è¡¨
    SOURCE_URLS = [ ]

# SOURCE_URLS = [
# "https://telegra.ph/BRbINz-09-05",
# "https://telegra.ph/mymUJf-08-28",
#
# ]

# æ–°çš„ä½œè€…ä¿¡æ¯
AUTHOR_NAME = "å¾¡å¥³å®«ç¦åˆ©å§¬é¢‘é“â†ç‚¹å‡»å…³æ³¨"
AUTHOR_URL = "https://erik7761269084-arch.github.io/-/%E7%A6%8F%E5%88%A9%E5%A7%AC%E9%A2%91%E9%81%93%E6%80%BB%E7%9B%AE%E5%BD%95/index.html"

# AUTHOR_NAME = "å¾¡å¥³å®«æ¼«ç”»é¢‘é“â†ç‚¹å‡»å…³æ³¨"
# AUTHOR_URL = "https://erik7761269084-arch.github.io/-/%E6%BC%AB%E7%94%BB%E9%A2%91%E9%81%93%E6%80%BB%E7%9B%AE%E5%BD%95/index.html"

# AUTHOR_NAME = "å¾¡å¥³å®«å°è¯´é¢‘é“â†ç‚¹å‡»å…³æ³¨"
# AUTHOR_URL = "https://erik7761269084-arch.github.io/-/%E5%B0%8F%E8%AF%B4%E9%A2%91%E9%81%93%E6%80%BB%E7%9B%AE%E5%BD%95/index.html"


ALLOWED_TAGS = {
    "p", "br", "strong", "b", "i", "em", "u", "a",
    "blockquote", "img", "video", "h3", "h4",
    "pre", "code", "ul", "ol", "li", "figure", "figcaption"
}

def clean_content(article):
    """æ¸…ç†æ­£æ–‡ï¼Œä¸ä¿ç•™åŸä½œè€…å’Œå‰¯æ ‡é¢˜"""
    # åˆ é™¤åŸä½œè€…æ®µè½
    for p in article.find_all("p"):
        if any(keyword in p.get_text() for keyword in ["å…³æ³¨é¢‘é“", "cosplayå†™çœŸé¢‘é“"]):
            p.decompose()

    # åˆ é™¤ headerã€h1/h2ã€address ç­‰
    for tag in article.find_all(["header", "h1", "h2", "address"]):
        tag.decompose()

    # æ¸…ç†ä¸æ”¯æŒæ ‡ç­¾
    for tag in article.find_all(True):
        if tag.name not in ALLOWED_TAGS:
            tag.unwrap()

    if switch_del_content_words:

        NEWALLOWED_TAGS = ["figure", "img"]

        # éå†æ‰€æœ‰æ ‡ç­¾
        for tag in article.find_all(True):
            if tag.name not in NEWALLOWED_TAGS:
                tag.decompose()  # åˆ é™¤æ ‡ç­¾åŠå†…å®¹

        # # åˆ é™¤ <strong>ã€<blockquote> é‡Œçš„å†…å®¹
        # for tag in article.find_all(["strong", "blockquote"]):
        #     tag.decompose()

    return "".join(str(child) for child in article.contents)

def extract_title(header_tag):
    """ä» header ä¸­æå–å¹²å‡€çš„æ ‡é¢˜ï¼Œåªä¿ç•™æ ¸å¿ƒæ–‡å­—"""
    if not header_tag:
        return "æ— æ ‡é¢˜"
    text = header_tag.get_text(strip=True)

    # å»æ‰å¹¿å‘Š/å‰¯æ ‡é¢˜/æ—¥æœŸç­‰
    text = re.split(r"cosplayå†™çœŸé¢‘é“|æ¯æ—¥ç™¾åˆæœ¬æ¨é€|é£æœˆæ–‡å­¦|â†ç‚¹å‡»å…³æ³¨|ç™¾ä¸‡å›¾é›†å…è´¹çœ‹|ğŸ‘‰ğŸ».*", text)[0].strip()

    # å†æ¬¡æˆªæ–­ï¼Œé˜²æ­¢æ¼æ‰å¹¿å‘Šè¯
    text = re.split(r"(cosplay|é¢‘é“|æ¯æ—¥ç™¾åˆæœ¬æ¨é€|é£æœˆæ–‡å­¦|Isabelle|http|â†ç‚¹å‡»å…³æ³¨|ç™¾ä¸‡å›¾é›†å…è´¹çœ‹|ğŸ‘‰ğŸ».*)", text, 1)[0].strip()

    # å®šä¹‰è¦åˆ é™¤çš„å¹¿å‘Šå…³é”®è¯ï¼ˆç›´æ¥åˆ‡æ‰ï¼Œä¸å½±å“åé¢å†…å®¹ï¼‰
    remove_keywords = [
        "[ä¸­å›½ç¿»è¨³]",
        "[æé»„ç¯å–µæ±‰åŒ–ç»„Ã—ç¢§è“æ¡£æ¡ˆç™¾åˆç»„Ã—ä¸€é’äºŒç™½æ±‰åŒ–ç»„]",
        "[Chinese]",
        "[ç½‘çº¢COSERå†™çœŸ]",
        "[ç½‘çº¢COS]",
        "[ç½‘çº¢COSER]",
        "[Cosplayå†™çœŸ]",
        "[COSç¦åˆ©]",
        "[æ¢¦ä¸å¥³ç¥MSLASS]",
        "[Digi-Gra]",
        "[LOVEPOP]",
        "[BLUECAKE]",
        "[Cosplay]",
        "[ç§€äººXiuRen]",
        "[ç¦åˆ©COS]",
        "[å–µç³–æ˜ ç”»]",
        "[èè‰COS]",
        "[å°¤èœœYouMi]",
        "[å°¤èœœYouMiabc]",
        "[çˆ±å°¤ç‰©Ugirls]",
        "[ç§€äººXIUREN]",
        "[å…”ç©æ˜ ç”»]",
        "[é£ä¹‹é¢†åŸŸ]",
        "[æ¨¡èŒƒå­¦é™¢MFStar]",
        "[[ç³–æœç”»æŠ¥CANDY]]",
        "[å—²å›¡å›¡FEILIN]",
        "[ç§€äººç½‘XiuRen]",
        "[ç¾åª›é¦†MyGirl]",
        "[å°¤ç‰©é¦†YouWu]",
        "[Bololiæ³¢èç¤¾]",
        "[MFèŒç¼š]",
        "[å¾¡å¥³éƒDKGirl]",
        "[Fantasy Factory]",
        "[SSAä¸ç¤¾]",
        "[èœœä¸MISSLEG]",
        "[Liguiä¸½æŸœ]",
        "[ç§€äººXiuRen]",
        "[è¯­ç”»ç•ŒXIAOYU]",
        "[ç§€äººXiuren]",
        "[èœœæ¡ƒç¤¾MiiTao]",
        "[çˆ±èœœç¤¾IMiss]",
        "[èŠ±æ¼¾HuaYang]",
        "[å°¤èœœèŸYouMi]",
        "[ç„¡ä¿®æ­£]",
        "[æš´ç¢§æ±‰åŒ–ç»„]",
        "[5DKä¸ªäººæ±‰åŒ–]",
        "[ä¸­å›½èª]",
        "[çŒ«èŒæ¦œMICAT]",
        "[å°¤æœUgirls]",
        "[FEILINå—²å›¡å›¡]",
        "[TASTEé¡½å‘³ç”Ÿæ´»]",
        "[å°¤æœåœˆçˆ±å°¤ç‰©]",
        "[Fantia]",
        "[Xiuren]",
        "fantia",
        "[]",
        "è¶…æ¸…",
        "COSå°‘å¥³",
        "å¯çˆ±äººæ°”Coser",
        "äººæ°”Coser",
        "å¾®åšCOSå¦¹å­",
        "COSç¦åˆ©",
        "COSå¦¹å­",
        "Coserå°å§å§",
        "äºŒæ¬¡å…ƒå¦¹å­",
        "å¾®åšäººæ°”Coser",
        "çˆ±ä¸½ä¸å†™çœŸ",
        "æ¸…çº¯å¦¹å­",
        "æ¸…çº¯å°‘å¥³",
        "æ‰“åŒ…åˆé›†",
        "æ€§æ„ŸåŠ¨æ¼«Coser@",
        "èŒå® åšä¸»",
        "å¾®åšèŒå¦¹",
        "ç¾å¥³ä¸»æ’­"
        "ç”µå–µå¥³ç¥",
        "å–µç³–æ˜ ç”»-",
        "@"
        "é©¬é‡Œå¥¥å°å±‹",
        "åŠ¨æ¼«åšä¸»",
        "å¯çˆ±å¦¹å­",
        "æ–—é±¼ä¸»æ’­",
        "äºŒæ¬¡å…ƒå·¨ä¹³ç¾å¥³",
        "äºŒæ¬¡å…ƒå·¨ä¹³",
        "é˜³å…‰ç¾å°‘å¥³",
        "èŒç³»å°å§å§",
        "å†™çœŸ",
        "å›¾é›†",
        "ã€ŠFantasy Factoryã€‹",
        "Cosplay",
        "äºŒæ¬¡å…ƒå°‘å¥³",
        "æ¬¡å…ƒå°‘å¥³",
        "å¾¡å¥³å®«æ¼«ç”»(å½©è‰²)",
        "å¾¡å¥³å®«æ¼«ç”»(é»‘ç™½)",
        "MyGirlç¾åª›é¦†",
        "äººæ°”Coser",
        "åŠ¨æ¼«åšä¸»"
    ]

    for kw in remove_keywords:
        text = text.replace(kw, "")

    # å»æ‰å¤šä½™ç©ºæ ¼
    return text.strip()

    return text


# ç™»å½• telegraph
telegraph = Telegraph()
telegraph.create_account(short_name="copybot")

# è¾“å‡º HTML æ–‡ä»¶
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
result_html = f"result_{timestamp}.html"

with open(result_html, "w", encoding="utf-8") as f_html:
    for url in SOURCE_URLS:
        try:
            res = requests.get(url)
            soup = BeautifulSoup(res.text, "html.parser")
            # soup.contents[2].contents[1].contents[3].contents[0]
            # è·å–å¹²å‡€æ ‡é¢˜
            header_tag = soup.find("header")
            if (
                    header_tag and
                    len(header_tag.contents) > 1 and
                    header_tag.contents[1].contents
            ):
                # æœ‰å†…å®¹å°±å–å®ƒ
                title = header_tag.contents[1].contents[0]
                title = extract_title(title)
            else:
                # å¦åˆ™è°ƒç”¨å‡½æ•°
                title = extract_title(header_tag)

            # æ¸…ç†æ­£æ–‡
            article = soup.find("article")
            content_html = clean_content(article)

            try:
                # åˆ›å»ºæ–°é¡µé¢
                response = telegraph.create_page(
                    title=title,
                    html_content=content_html,
                    author_name=AUTHOR_NAME,
                    author_url=AUTHOR_URL
                )
            except Exception as e:
                err_msg = str(e)
                if re.search(r"Flood control exceeded", err_msg):
                    print(f"âŒ Flood æ§åˆ¶è§¦å‘ï¼Œåœæ­¢ç¨‹åº: {url}")
                    sys.exit(1)  # ç›´æ¥ç»ˆæ­¢æ•´ä¸ªè„šæœ¬
                else:
                    print(f"âŒ å¤„ç† {url} å‡ºé”™: {e}")

            new_url = f"https://telegra.ph/{response['path']}"
            print(f"âœ… {title} -> {new_url}")

            # å†™å…¥ HTML æ–‡ä»¶
            f_html.write(f'<p><a href="{new_url}">{title}</a></p>\n')

        except Exception as e:
            print(f"âŒ å¤„ç† {url} å‡ºé”™: {e}")
            f_html.write(f"<p>âŒ å¤„ç† {url} å‡ºé”™: {e}</p>\n")

print(f"âœ… å·²ç”Ÿæˆ HTML æ–‡ä»¶: {result_html}")
